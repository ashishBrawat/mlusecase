# new_cluster: &new_cluster
#   new_cluster:
#     num_workers: 1
#     spark_version: 13.3.x-cpu-ml-scala2.12
#     node_type_id: Standard_D3_v2
#     custom_tags:
#       clusterSource: mlops-stacks_0.3

# common_permissions: &permissions
#   permissions:
#     - level: CAN_VIEW
#       group_name: users

# resources:
#   jobs:
#     write_feature_table_job:
#       name: ${bundle.target}-ml_usecase-write-feature-table-job
#       job_clusters:
#         - job_cluster_key: write_feature_table_job_cluster
#           <<: *new_cluster
#       tasks:
#         - task_key: PickupFeatures
#           job_cluster_key: write_feature_table_job_cluster
#           notebook_task:
#             #notebook_path: ../feature_engineering/notebooks/feature_store.py # regression
#             notebook_path: ../feature_engineering/notebooks/clustering_feature_store.py # clustering
#             base_parameters:
#               # TODO modify these arguments to reflect your setup.
#               #input_table_path: dev_ashish.ashish_mlops.regression_sample  # regression
#               input_table_path: dbfs:/FileStore/demo_data/Iris.csv # clustering
#               # TODO: Empty start/end dates will process the whole range. Update this as needed to process recent data.
#               #input_start_date: ""
#               #input_end_date: ""
#               #timestamp_column: tpep_pickup_datetime
#               #output_table_name: ${bundle.target}.ml_usecase.features # regression
#               output_table_name: ${bundle.target}.clustering_demo_mlops.iris_sepal_features # clustering
#               #features_transform_module: pickup_features
#               #primary_keys: zip
#               features_transform_module: sepal_features
#               primary_keys: Id
#               # git source information of current ML resource deployment. It will be persisted as part of the workflow run
#               git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
#         #- task_key: DropoffFeatures
#           #job_cluster_key: write_feature_table_job_cluster
#           #notebook_task:
#             #notebook_path: ../feature_engineering/notebooks/GenerateAndWriteFeatures.py
#             #base_parameters:
#               ## TODO: modify these arguments to reflect your setup.
#               #input_table_path: /databricks-datasets/nyctaxi-with-zipcodes/subsampled
#               ## TODO: Empty start/end dates will process the whole range. Update this as needed to process recent data.
#               #input_start_date: ""
#               #input_end_date: ""
#               #timestamp_column: tpep_dropoff_datetime
#               #output_table_name: ${bundle.target}.ml_usecase.trip_dropoff_features
#               #features_transform_module: dropoff_features
#               #primary_keys: zip
#               ## git source information of current ML resource deployment. It will be persisted as part of the workflow run
#               #git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
#       # schedule:
#       #   quartz_cron_expression: "0 0 7 * * ?" # daily at 7am
#       #   timezone_id: UTC
#       # <<: *permissions
#       # If you want to turn on notifications for this job, please uncomment the below code,
#       # and provide a list of emails to the on_failure argument.
#       #
#       #  email_notifications:
#       #    on_failure:
#       #      - first@company.com
#       #      - second@company.com

#       # Clustering
#       - task_key: PetalFeatures
#           job_cluster_key: write_feature_table_job_cluster
#           notebook_task:
#             notebook_path: ../feature_engineering/notebooks/GenerateAndWriteFeatures.py
#             base_parameters:
#               # TODO: modify these arguments to reflect your setup.
#               input_table_path: dbfs:/FileStore/demo_data/Iris.csv
#               # TODO: Empty start/end dates will process the whole range. Update this as needed to process recent data.
#               input_start_date: ""
#               input_end_date: ""
#               # timestamp_column: tpep_dropoff_datetime
#               output_table_name: ${bundle.target}.clustering_demo_mlops.iris_petal_features
#               features_transform_module: petal_features
#               primary_keys: Id
#               # git source information of current ML resource deployment. It will be persisted as part of the workflow run
#               git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
#       parameters:
#       - name: env
#         default: ${bundle.target}
#       - name: input_table_path
#         default: dbfs:/FileStore/demo_data/Iris.csv
#       # schedule:
#       #   quartz_cron_expression: "0 0 7 * * ?" # daily at 7am
#       #   timezone_id: UTC
#       <<: *permissions
#       # If you want to turn on notifications for this job, please uncomment the below code,
#       # and provide a list of emails to the on_failure argument.
#       #
#       email_notifications:
#         on_success:
#           - ashish.b.rawat@koantek.com
#         on_start:
#           - ashish.b.rawat@koantek.com
#         on_failure:
#           - ashish.b.rawat@koantek.com

new_cluster: &new_cluster
  new_cluster:
    num_workers: 1
    spark_version: 13.3.x-cpu-ml-scala2.12
    node_type_id: Standard_D3_v2
    custom_tags:
      clusterSource: mlops-stack/0.2

common_permissions: &permissions
  permissions:
    - level: CAN_VIEW
      group_name: users

resources:
  jobs:
    write_feature_table_job:
      name: ${bundle.target}-clustering_demo_mlops-write-feature-table-job
      job_clusters:
        - job_cluster_key: write_feature_table_job_cluster
          <<: *new_cluster
      tasks:
        - task_key: SepalFeatures
          job_cluster_key: write_feature_table_job_cluster
          notebook_task:
            notebook_path: ../feature_engineering/notebooks/GenerateAndWriteFeatures.py
            base_parameters:
              # TODO modify these arguments to reflect your setup.
              input_table_path: dbfs:/FileStore/demo_data/Iris.csv
              # TODO: Empty start/end dates will process the whole range. Update this as needed to process recent data.
              input_start_date: ""
              input_end_date: ""
              # timestamp_column: tpep_pickup_datetime
              output_table_name: ${bundle.target}.clustering_demo_mlops.iris_sepal_features
              features_transform_module: sepal_features
              primary_keys: Id
              # git source information of current ML resource deployment. It will be persisted as part of the workflow run
              git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
        - task_key: PetalFeatures
          job_cluster_key: write_feature_table_job_cluster
          notebook_task:
            notebook_path: ../feature_engineering/notebooks/GenerateAndWriteFeatures.py
            base_parameters:
              # TODO: modify these arguments to reflect your setup.
              input_table_path: dbfs:/FileStore/demo_data/Iris.csv
              # TODO: Empty start/end dates will process the whole range. Update this as needed to process recent data.
              input_start_date: ""
              input_end_date: ""
              # timestamp_column: tpep_dropoff_datetime
              output_table_name: ${bundle.target}.clustering_demo_mlops.iris_petal_features
              features_transform_module: petal_features
              primary_keys: Id
              # git source information of current ML resource deployment. It will be persisted as part of the workflow run
              git_source_info: url:${bundle.git.origin_url}; branch:${bundle.git.branch}; commit:${bundle.git.commit}
      parameters:
      - name: env
        default: ${bundle.target}
      - name: input_table_path
        default: dbfs:/FileStore/demo_data/Iris.csv
      # schedule:
      #   quartz_cron_expression: "0 0 7 * * ?" # daily at 7am
      #   timezone_id: UTC
      <<: *permissions
      # If you want to turn on notifications for this job, please uncomment the below code,
      # and provide a list of emails to the on_failure argument.
      #
      email_notifications:
        on_success:
          - ashish.b.rawatkoantek.com
        on_start:
          - ashish.b.rawatkoantek.com
        on_failure:
          - ashish.b.rawatkoantek.com
